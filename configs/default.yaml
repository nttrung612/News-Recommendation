seed: 42

data:
  train_behaviors: data/MINDsmall_train/behaviors.tsv
  train_news: data/MINDsmall_train/news.tsv
  dev_behaviors: data/MINDsmall_dev/behaviors.tsv
  dev_news: data/MINDsmall_dev/news.tsv
  cache_dir: data/cache
  max_history: 20
  max_seq_len: 32

model:
  name: fastformer  # registry key; can swap to other models later
  pretrained_model_name: distilroberta-base  # smaller encoder for 4GB VRAM
  tokenizer_name: distilroberta-base
  embed_dim: 192
  dropout: 0.2

train:
  batch_size: 8
  num_workers: 4
  epochs: 2
  learning_rate: 2.0e-5
  weight_decay: 0.01
  warmup_steps: 0
  neg_k: 3
  grad_accum_steps: 1
  fp16: true
  sample_ratio: 0.05   # use 10% of training impressions for quick demo
  log_interval: 500   # steps between loss logs
  save_every_steps: 1000  # set 0 or null to disable

eval:
  batch_size: 4
  sample_ratio: 0.05   # keep full dev by default
