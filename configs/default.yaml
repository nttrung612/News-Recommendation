seed: 42

data:
  train_behaviors: data/MINDsmall_train/behaviors.tsv
  train_news: data/MINDsmall_train/news.tsv
  dev_behaviors: data/MINDsmall_dev/behaviors.tsv
  dev_news: data/MINDsmall_dev/news.tsv
  cache_dir: data/cache
  max_history: 50
  max_seq_len: 64

model:
  name: fastformer  # registry key; can swap to other models later
  pretrained_model_name: deberta-v3-base
  tokenizer_name: deberta-v3-base
  embed_dim: 320
  dropout: 0.1
  use_token_fastformer: true  # token-level pooling instead of plain [CLS]
  category_dim: 32            # set 0 to disable category embedding
  subcategory_dim: 16         # set 0 to disable subcategory embedding

train:
  batch_size: 64
  num_workers: 6
  epochs: 4
  learning_rate: 3.0e-5
  weight_decay: 0.01
  warmup_steps: 0.05
  neg_k: 4
  grad_accum_steps: 1
  fp16: true
  sample_ratio: 0.1   # use 10% of training impressions for quick demo
  log_interval: 200   # steps between loss logs
  save_every_steps: 1000  # set 0 or null to disable

eval:
  batch_size: 128
  sample_ratio: 0.1   # use 10% of dev impressions for quick demo